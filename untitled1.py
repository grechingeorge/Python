# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bG8O6q5ReaCEy3zAS_jeqzqBTRtXnJyf

# Введение

HR-аналитики компании «Работа с заботой» помогают бизнесу оптимизировать управление персоналом: бизнес предоставляет данные, а аналитики предлагают, как избежать финансовых потерь и оттока сотрудников. В этом HR-аналитикам пригодится машинное обучение, с помощью которого получится быстрее и точнее отвечать на вопросы бизнеса. Компания предоставила данные с характеристиками сотрудников компании. Среди них — уровень удовлетворённости сотрудника работой в компании. Эту информацию получили из форм обратной связи: сотрудники заполняют тест-опросник, и по его результатам рассчитывается доля их удовлетворённости от 0 до 1, где 0 — совершенно неудовлетворён, 1 — полностью удовлетворён.

Цель проекта: Разработать модели машинного обучения для предсказания уровня удовлетворенности сотрудника работой в компании и для прогнозирования факта увольнения сотрудника на основе предоставленных данных заказчика.


Этапы проекта Задача 1:
1. Загрузка и изучение данных
2. Предобработка данных
3. Исследовательский анализ данных
4. Подготовка данных и обучение моделей
"""

!pip install -U scikit-learn -q
!pip install shap -q
!pip install phik -q

import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import shap
import phik

from sklearn.compose import ColumnTransformer
from sklearn.impute import  SimpleImputer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder,  RobustScaler
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.metrics import make_scorer
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')
import transformers


RANDOM_STATE = 42

def duplicates(columns, df):
    for column in columns:
        unique_values = df[column].unique()
        print(f'Уникальные значения в колонке {column}: {unique_values}')

def cat_num_col(df):
    num_col_names = list(df.select_dtypes(include=['int64', 'float64']).columns)
    cat_col_names = list(df.select_dtypes(include=['object']).columns)
    num_col_names.remove('id')
    return num_col_names, cat_col_names

def plot_phik_heatmap(df, interval_cols):
    # Вычисление phik_matrix
    phik_matrix = df.phik_matrix(interval_cols=interval_cols)

    # Использование цветовой карты RdYlBu для лучшей визуализации
    cmap = sns.diverging_palette(220, 20, as_cmap=True)

    # Создание тепловой карты на основе phik_matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(phik_matrix, annot=True, cmap=cmap, center=0, square=True, linewidths=.5)
    plt.title('Phik. Тепловая карта')
    plt.show()

"""### Загрузка данных

"""

try:
    train_job = pd.read_csv('https://code.s3.yandex.net/datasets/train_job_satisfaction_rate.csv')
    test_features = pd.read_csv('https://code.s3.yandex.net/datasets/test_features.csv')
    test_target = pd.read_csv('https://code.s3.yandex.net/datasets/test_target_job_satisfaction_rate.csv')
except FileNotFoundError:
    print('Ошибка')

display(train_job.head())
display(test_features.head())
display(test_target.head())

train_job.info()

"""Тренировочные данные содержат 4000 строк, 10 колонок. В файле следующие столбцы

* id — уникальный идентификатор сотрудника;
* dept — отдел, в котором работает сотрудник;
* level — уровень занимаемой должности;
* workload — уровень загруженности сотрудника;
* employment_years — длительность работы в компании (в годах);
* last_year_promo — показывает, было ли повышение за последний год;
* last_year_violations — показывает, нарушал ли сотрудник трудовой договор за последний год;
* supervisor_evaluation — оценка качества работы сотрудника, которую дал руководитель;
* salary — ежемесячная зарплата сотрудника;
* job_satisfaction_rate — уровень удовлетворённости сотрудника работой в компании, целевой признак.

В качестве данных у нас 3 датафрейма. Тренирочая выборка, признаки тестовой выборки и целевой признак тестовой выборки. Все типы данных соотвествуют тем, что были даны в описании к названиям колонок. В датафреймах test_features и train_job есть пропуски

### Предобработка данных
"""

print(f'количество пропусков:\n{train_job.isna().sum()}')
print(f'количество пропусков:\n{test_features.isna().sum()}')

duplicates(['dept', 'level'], train_job)

data_frames = [test_target, test_features, train_job]

for data in data_frames:
    data_name = [key for key, value in locals().items() if value is data][0]
    print(f'Количество дубликатов в датафрейме {data_name}: {data.duplicated().sum()}')
    if data.duplicated().sum() > 0:
        data.drop_duplicates(inplace=True)
        print(f'Количество дубликатов в датафрейме {data_name}: {data.duplicated().sum()}')

num_col_train_job, cat_col_train_job = cat_num_col(train_job)
num_col_test_features, cat_col_test_features = cat_num_col(test_features)

print('df_train_job_satisfaction_rate')
for col_name in cat_col_train_job:
    print(f'Количество уникальны значений в столбце {col_name}: {train_job[col_name].unique()}')
print()
print('df_test_features')
for col_name in cat_col_test_features:
    print(f'Количество уникальны значений в столбце {col_name}: {test_features[col_name].unique()}')

test_features['dept'].replace(' ', np.nan, inplace=True)
test_features['workload'].replace(' ', np.nan, inplace=True)

"""**Вывод 1.2**
* В данных нет дубликатов.
* В столбцах workload и dept датафрейма test_features были обнаружены пустые строки. Заменим их на nan для последующей обработки в пайплайне.

### Исследовательский анализ данных
"""

for i in train_job[num_col_train_job].columns:
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))
    axes[0].hist(train_job[i], bins=30)
    axes[0].set_title('Гистограмма распределения')
    axes[0].set_xlabel(i)

    axes[1].boxplot(train_job[i])
    axes[1].set_xlabel(i)
    axes[1].set_title('Ящик с усами')
    plt.show()

for column in train_job[cat_col_train_job].columns:
    train_job.groupby(column).count().plot(kind='pie', y='id', autopct='%1.0f%%', figsize=(8,8))

"""В данных train_job есть выбросы в зарплате и оценка качества работы сотрудника, которую дал руководитель. Так как это сотрудники и для модели будет лучше уметь обрабатывать такие выбросы чем не уметь - мы оставим выбросы, но будем использовать скалер, который устойчив к выбросам. У тренировочных данных разброс зарлаты от 0 до 10 лет, оценка качества работы сотрудника, которую дал руководитель от 1 до 5, больше всего 4.0, зарпалата имеет большой разброс, 10.000 - 100.000 рублей. Удволетворенность работы от 0 и до 1, распределение волнообразное, 2 горба. БОльший горб находится в районе 0.8 баллов. Больше всего сотрудников из отдела продаж, меньше из hr. В компании больше всего джунов, меньше синьоров, у большинства средняя загруженность. У большинства не было повышения в последний год и не было нарушего трудового договора."""

for i in test_features[num_col_test_features].columns:
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))
    axes[0].hist(test_features[i], bins=30)
    axes[0].set_title('Гистограмма распределения')
    axes[0].set_xlabel(i)

    axes[1].boxplot(test_features[i])
    axes[1].set_xlabel(i)
    axes[1].set_title('Ящик с усами')
    plt.show()

for column in test_features[cat_col_test_features].columns:
    test_features.groupby(column).count().plot(kind='pie', y='id', autopct='%1.0f%%', figsize=(8,8))

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))
    axes[0].hist(test_target['job_satisfaction_rate'], bins=30)
    axes[0].set_title('Гистограмма распределения')
    axes[0].set_xlabel('job_satisfaction_rate')

    axes[1].boxplot(test_target['job_satisfaction_rate'])
    axes[1].set_xlabel('job_satisfaction_rate')
    axes[1].set_title('Ящик с усами')
    plt.show()

"""**Вывод** Мы обнаружили выбросы в столбцах supervisor_evaluation и salary, которые скорей всего связаны с тем, что в компании предствлено меньше всего специалистовы высого уровня (Senior), чья зп может достигать очень большыих значений. Оставим эти выбросы, для более качественного обучения модели"""

phik_train_job = train_job.set_index('id')
plot_phik_heatmap(phik_train_job, num_col_train_job)

phik_test_features = test_features.set_index('id')
plot_phik_heatmap(phik_test_features, num_col_test_features)

"""По диаграмме видно, что мжеду workload и salary, level и salary, supervisor_evaluation и job_satisfaction имеется достаточно высокая корреляция 72-79%

### Подготовка данных
"""

#список столбцов для кодирования
ord_cols = ['level', 'workload', 'last_year_promo', 'last_year_violations']
ohe_cols = ['dept']

train_features = train_job.drop(['job_satisfaction_rate'], axis=1)
train_target = train_job['job_satisfaction_rate']

test_combined = test_features.merge(test_target, on='id')
print(f'количество дубликатов:{test_combined.duplicated().sum()}')
print()
print(f'количество пропусков:\n{test_combined.isna().sum()}')
print()
print(f'размерность датафрейма: {test_combined.shape}')
test_combined_target = test_combined['job_satisfaction_rate']
test_combined_features = test_combined.drop('job_satisfaction_rate', axis=1)

"""В качестве моделей будем использовать DecisionTreeRegressor, LinearRegression

Для оценки качества моделей будем использовать метрику SMAPE, функцию для нее напишем самостоятельно. Критерий успеха SMAPE <= 15
"""

#обрабатываем качественные признаки OneHotEncoder
ohe_pipe = Pipeline([
    ('simpleImputer_ohe', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(drop='first', handle_unknown='error', sparse_output=False))
])
#обрабатываем качественные признаки OrdinalEncoder
ord_pipe = Pipeline([
    ('simpleImputer_before_ord', SimpleImputer(strategy='most_frequent')),
    ('ord', OrdinalEncoder(categories=[['junior', 'middle', 'sinior'],
                                       ['low', 'medium', 'high'],
                                       ['no', 'yes'],
                                       ['no', 'yes']],
                                         handle_unknown='use_encoded_value', unknown_value=np.nan)),
    ('simpleImputer_after_ord', SimpleImputer(strategy='most_frequent'))
])

#обрабатываем количественные признаки RobustScaler
numeric_pipe = Pipeline(steps=[
    ('scaler', RobustScaler())
])

# определение ColumnTransformer
data_preprocessor = ColumnTransformer(transformers=[
    ('ohe', ohe_pipe, ohe_cols),
    ('ord', ord_pipe, ord_cols),
    ('num', numeric_pipe, num_col_test_features)
], remainder='passthrough')

# создание основного пайплайна
pipe_final = Pipeline([
    ('preprocessor', data_preprocessor),
    ('models', DecisionTreeRegressor(random_state=RANDOM_STATE))
])
#создание сетки моделей
param_grid = [
    {'models': [DecisionTreeRegressor(random_state=RANDOM_STATE)],
     'models__max_depth': range(2, 250),
     'models__max_features': range(2,50),
     'models__min_samples_leaf': range(1,5),
     'models__min_samples_split': range(2,20)
    },

    {
        'models': [LinearRegression()],
    }
]

# определение функции SMAPE
def smape(y_test, pred):
    metric = 100/len(y_test) * np.sum(2 * np.abs(pred - y_test) / (np.abs(y_test) + np.abs(pred)))
    return metric

# создание объекта RandomizedSearchCV
random_search = RandomizedSearchCV(
    pipe_final,
    param_grid,
    cv=10,
    scoring=make_scorer(smape, greater_is_better=False),
    n_jobs=-1,
    random_state=RANDOM_STATE
    )

# подгонка модели
random_search.fit(train_features, train_target)

# вывод лучшей модели и ее параметров
print('Лучшая модель и ее параметры:\n\n', random_search.best_estimator_)
print('Метрика лучшей модели на тренировочной выборке:', random_search.best_score_)

pred = random_search.predict(test_combined_features)
score=make_scorer(smape)
smape_score = score(random_search.best_estimator_, test_combined_features, test_combined_target)
print(f'Метрика SMAPE на тестовой выборке: {smape_score}')

"""**Вывод**

- Метрика SMAPE на тестовой выборке: 14.05

Лучшая модель с ее гиперпараметрами:
- DecisionTreeRegressor
- max_depth=41,
- max_features=37,
- min_samples_leaf=2,
- min_samples_split=17,
- random_state=42


Лучшей метрикой стало дерево решений. Хороший результат дерева решений может быть обоснован спецификой нашего набора данных. Деревья решений могут хорошо обрабатывать нелинейные отношения между переменными, а также способны автоматически преобразовывать и выбирать признаки для разделения, что может быть полезным в нашем наборе данных. Дерево решений более сильная модель чем логистическая, у этой модели больше внутренних настроек, которые можно изменять.
"""